{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3c57376-eac3-4b01-8f02-e77581699932",
   "metadata": {},
   "source": [
    "# Set up project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc2ed3-2970-4afa-9f3a-b94008f1da71",
   "metadata": {},
   "source": [
    "## Create bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e349c3e-e362-4c16-815c-386f41e23b80",
   "metadata": {},
   "source": [
    "In the console we will do the following:\n",
    "1. Navigate to Cloud Storage\n",
    "1. Go to Buckets\n",
    "1. Click Create\n",
    "1. Follow the prompts, etc.\n",
    "\n",
    "Instead, we will use the CLI to do this quickly with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b8cbc-080f-41cd-804a-1805fcf7ddbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud storage buckets create \"gs://mlops-maturity-tutorial-level0\" --location='us-west1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30495a7a-73f1-4bd8-abe4-02bbdc6204ab",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65058147-dd20-4db8-ab57-3c3ac2128389",
   "metadata": {},
   "source": [
    "## Develop model training script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a59ce9-e068-4b0d-af9e-5054ebd5114d",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, we will define a very simple model that trains and saves a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17756023-f760-433f-ae38-b6f1c3ad5f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from google.cloud import storage\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Load the data\n",
    "iris = load_iris()\n",
    "\n",
    "# Train a basic Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(iris.data, iris.target)\n",
    "print(\"Model fit complete\")\n",
    "\n",
    "# Save the model\n",
    "with open(\"model.pkl\", 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"Model saved to GCS\")\n",
    "\n",
    "# Upload model artifact to Cloud Storage\n",
    "model_directory = 'gs://mlops-maturity-tutorial-level0'\n",
    "storage_path = os.path.join(model_directory, 'model.pkl')\n",
    "blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())\n",
    "blob.upload_from_filename('model.pkl')\n",
    "print(\"Model artifact pushed\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c543f0-8a98-41c4-b2a5-cc6f7041154d",
   "metadata": {},
   "source": [
    "## Submit training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76629b20-8c63-477e-abf6-7c38fe0433b2",
   "metadata": {},
   "source": [
    "While yes, the above script runs here in a notebook, let's demonstrate how to use Vertex AI training to do this instead. This is especially the case for larger models that require a GPU or a fleet of GPUs where you don't want the Notebook instance to carry that load.\n",
    "\n",
    "In the console, we would do the following:\n",
    "1. Navigate to Vertex AI\n",
    "1. Go to Training\n",
    "1. Click Train New Model\n",
    "1. Follow the prompts, etc.\n",
    "\n",
    "Instead, we will use the Python SDK in order to automate the process. To do this, we do the following:\n",
    "1. Save the above script as a .py (we have already done this in src/)\n",
    "1. Use the Python SDK to create and submit a CustomTrainingJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d8bad-ff8d-4678-926c-838f3417c13d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import CustomTrainingJob\n",
    "\n",
    "job = CustomTrainingJob(\n",
    "    display_name='iris_level0',\n",
    "    script_path='src/train.py',\n",
    "    container_uri='us-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest',\n",
    "    staging_bucket='mlops-maturity-tutorial-level0',\n",
    "    location='us-west1'\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225727b2-0b26-40c6-bdb3-089ea38db50b",
   "metadata": {},
   "source": [
    "# Model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a57f7-16eb-4bd4-8386-603af9732679",
   "metadata": {},
   "source": [
    "## Wrap model artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff74729-36ba-4278-84c7-4e19b749a023",
   "metadata": {},
   "source": [
    "In the above script, we trained a model and outputted the model artifact as a model.pkl. Here we will load the model.pkl into an inference container.\n",
    "\n",
    "In the console, we would do the following:\n",
    "1. Navigate to Vertex AI\n",
    "1. Go to Model Registry\n",
    "1. Click Import\n",
    "1. Follow the prompts\n",
    "\n",
    "Instead, we will use the Python SDK in order to automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ea4ba-a044-4c78-a11b-be44b21acbc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import Model\n",
    "\n",
    "model = Model.upload(\n",
    "    display_name='iris_level0',\n",
    "    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest',\n",
    "    artifact_uri='gs://mlops-maturity-tutorial-level0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9235c6c-4271-4f82-91e4-5ae10e1bbd63",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7355b6bb-ffcd-4f8c-90b7-c435554a540a",
   "metadata": {},
   "source": [
    "Now we can deploy the trained model to an endpoint.\n",
    "\n",
    "In the console, we would do the following:\n",
    "1. Navigate to Vertex AI\n",
    "1. Go to Model Registry\n",
    "1. Find and click on the new model\n",
    "1. Go to Deploy & Test\n",
    "1. Click Deploy to Endpoint\n",
    "1. Follow the prompts\n",
    "\n",
    "Instead, we will use the Python SDK in order to automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166423d-c067-4aea-a23b-1e493d4e0968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645184e-eee4-48ad-b79f-0b337aed06b6",
   "metadata": {},
   "source": [
    "# Use endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c82fd2-20de-46aa-add7-1c25d4717c04",
   "metadata": {},
   "source": [
    "Finally, we can send a payload to the deployed endpoint.\n",
    "\n",
    "In the console, we would do the following:\n",
    "1. Navigate to Vertex AI\n",
    "1. Go to Model Registry\n",
    "1. Find and click on the new model\n",
    "1. Go to Deploy & Test\n",
    "1. Select the new endpoint\n",
    "1. Test the model with the input box.\n",
    "\n",
    "Instead, we will use the Python SDK in order to automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e96a84-53f8-470e-96fd-88a5dfbdcf2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instances = [\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 5]\n",
    "]\n",
    "\n",
    "responses = endpoint.predict(instances)\n",
    "\n",
    "print(responses.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585cdf6-bebf-4f04-953c-717f0fbf97f7",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d7cec-ec09-44ad-9a04-8884a1e5d5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Undeploy endpoint\n",
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e3fd9-e1de-45a6-a276-26e46788d611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
